"""
Web Scraper fÃ¼r die FÃ¶rderdatenbank des Bundes
https://www.foerderdatenbank.de/

Dieser Scraper sammelt FÃ¶rderprogramme und bereitet sie fÃ¼r GrantGPT auf.
"""
import requests
from bs4 import BeautifulSoup
import json
import time
from typing import List, Dict, Any
from datetime import datetime


class FoerderdatenbankScraper:
    """
    Scraper fÃ¼r die offizielle FÃ¶rderdatenbank des Bundes
    
    WICHTIG: 
    - Robots.txt beachten
    - Rate-Limiting einhalten (max 1 Request/Sekunde)
    - User-Agent setzen
    """
    
    def __init__(self):
        self.base_url = "https://www.foerderdatenbank.de"
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "GrantGPT-Bot/1.0 (FÃ¶rdermittel-Aggregator; info@grantgpt.de)"
        })
    
    def search_programs(
        self, 
        query: str = "", 
        region: str = "bundesweit",
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        Suche nach FÃ¶rderprogrammen
        
        Args:
            query: Suchbegriff (z.B. "Innovation", "Digitalisierung")
            region: Region (z.B. "bundesweit", "Bayern", "NRW")
            limit: Max. Anzahl Ergebnisse
            
        Returns:
            Liste von FÃ¶rderprogrammen
        """
        programs = []
        
        # TODO: Echte API-Calls implementieren
        # Die FÃ¶rderdatenbank hat vermutlich eine Such-API oder HTML-Struktur
        
        print(f"Scraping FÃ¶rderdatenbank fÃ¼r: {query} in {region}")
        
        # Beispiel-Struktur fÃ¼r gescrapte Programme
        example_program = {
            "id": "foerderdatenbank-12345",
            "source": "foerderdatenbank.de",
            "name": "Beispiel-FÃ¶rderprogramm",
            "type": "federal",  # oder "state", "eu"
            "category": "innovation",
            "description": "Beschreibung des Programms...",
            "max_funding": 500000,
            "min_funding": 50000,
            "min_own_contribution_percent": 15,
            "eligibility": [
                "KMU mit Sitz in Deutschland",
                "Innovative Technologie",
            ],
            "requirements": [
                "Detaillierter Projektantrag",
                "Finanzplan",
            ],
            "deadline": "2025-12-31T23:59:59Z",
            "is_continuous": False,
            "website_url": "https://www.foerderdatenbank.de/...",
            "contact_info": {
                "organization": "FÃ¶rderorganisation",
                "email": "info@beispiel.de",
                "phone": "+49 ...",
            },
            "scraped_at": datetime.now().isoformat(),
        }
        
        # Rate Limiting
        time.sleep(1)
        
        return programs
    
    def scrape_all_programs(self, output_file: str = "all_grants.json"):
        """
        Scrapt alle verfÃ¼gbaren FÃ¶rderprogramme
        
        STRATEGIE:
        1. Alle Kategorien durchgehen (Innovation, Digitalisierung, etc.)
        2. Alle Regionen (Bund, LÃ¤nder, EU)
        3. Duplikate entfernen
        4. In JSON speichern
        """
        all_programs = []
        
        categories = [
            "innovation",
            "digitalisierung", 
            "forschung",
            "gruendung",
            "export",
            "umwelt",
            "energie",
        ]
        
        regions = [
            "bundesweit",
            "baden-wuerttemberg",
            "bayern",
            "berlin",
            "brandenburg",
            "bremen",
            "hamburg",
            "hessen",
            "niedersachsen",
            "nrw",
            "rheinland-pfalz",
            "saarland",
            "sachsen",
            "sachsen-anhalt",
            "schleswig-holstein",
            "thueringen",
            "eu",
        ]
        
        print(f"Scraping {len(categories)} Kategorien x {len(regions)} Regionen...")
        
        for category in categories:
            for region in regions:
                print(f"\nâ†’ {category} in {region}")
                programs = self.search_programs(
                    query=category, 
                    region=region,
                    limit=50
                )
                all_programs.extend(programs)
                
                # Rate Limiting: 1 Request/Sekunde
                time.sleep(1)
        
        # Duplikate entfernen (basierend auf ID)
        unique_programs = {p["id"]: p for p in all_programs}.values()
        
        print(f"\nâœ… {len(unique_programs)} einzigartige Programme gefunden")
        
        # Speichern
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(list(unique_programs), f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ Gespeichert: {output_file}")
        
        return list(unique_programs)


def main():
    """
    Hauptfunktion zum Scrapen und Importieren
    """
    scraper = FoerderdatenbankScraper()
    
    print("=" * 60)
    print("FÃ¶rderdatenbank.de Scraper")
    print("=" * 60)
    
    # Alle Programme scrapen
    programs = scraper.scrape_all_programs(
        output_file="/app/data/grants/foerderdatenbank_all.json"
    )
    
    print(f"\nðŸŽ‰ Fertig! {len(programs)} Programme bereit fÃ¼r Import in GrantGPT")
    
    # TODO: Automatisch in Qdrant embedden
    # from app.services.embeddings import embedding_service
    # from app.services.qdrant_service import qdrant_service


if __name__ == "__main__":
    main()

